---
title: "Point-of-Interest Type Prediction using Text and Images"
collection: publications
permalink: https://arxiv.org/pdf/2109.00602.pdf
excerpt: 'We propose a model for POI type prediction combining text and image using a modality gate to control the amount of information needed from the text and image, and
a cross-attention mechanism to learn cross-modal interactions.

- üìú [Pre-print](https://arxiv.org/pdf/2109.00602.pdf) üóÉÔ∏è [Data and Code](https://github.com/danaesavi/poi-type-prediction)' 
venue: 'EMNLP'
---

**Abstract**

Point-of-interest (POI) type prediction is the
task of inferring the type of a place from where
a social media post was shared. Inferring a
POI‚Äôs type is useful for studies in computational social science including sociolinguistics,
geosemiotics, and cultural geography, and has
applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their
semiotic relationships with one another, shape
communication and interactions in social media. This paper presents a study on POI type
prediction using multimodal information from
text and images available at posting time. For
that purpose, we enrich a currently available
data set for POI type prediction with the images that accompany the text messages. Our
proposed method extracts relevant information
from each modality to effectively capture interactions between text and image achieving
a macro F1 of 47.21 across eight categories
significantly outperforming the state-of-the-art
method for POI type prediction based on textonly methods. Finally, we provide a detailed
analysis to shed light on cross-modal interactions and the limitations of our best performing model.

- üìú [Pre-print](https://arxiv.org/pdf/2109.00602.pdf) üóÉÔ∏è [Data and Code](https://github.com/danaesavi/poi-type-prediction)
